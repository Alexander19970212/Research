{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.distributions import Categorical\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple, Dict, Optional, Callable, Type, Any\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from gymcad_1d import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "simulator = BoxCad_1d() #(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "simulator.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterationBuffer(Dataset):\n",
    "    \"\"\"Buffer for experience replay\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Initialize `IterationBuffer`\"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.next_baselines = None\n",
    "        self.nullify_buffer()\n",
    "\n",
    "    def nullify_buffer(self) -> None:\n",
    "        \"\"\"Clear all buffer data\"\"\"\n",
    "\n",
    "        self.episode_ids = []\n",
    "        self.observations = []\n",
    "        self.actions = []\n",
    "        self.running_objectives = []\n",
    "        self.step_ids = []\n",
    "        self.total_objectives = None\n",
    "        self.baselines = None\n",
    "\n",
    "    def add_step_data(\n",
    "        self,\n",
    "        observation: np.array,\n",
    "        action: np.array,\n",
    "        running_objective: float,\n",
    "        step_id: int,\n",
    "        episode_id: int,\n",
    "    ):\n",
    "        \"\"\"Add step data to experience replay\n",
    "\n",
    "        Args:\n",
    "            observation (np.array): current observation\n",
    "            action (np.array): current action\n",
    "            running_objective (float): current running objective\n",
    "            step_id (int): current step\n",
    "            episode_id (int): current episode\n",
    "        \"\"\"\n",
    "        self.observations.append(observation)\n",
    "        self.actions.append(action)\n",
    "        self.running_objectives.append(running_objective)\n",
    "        self.episode_ids.append(int(episode_id))\n",
    "        self.step_ids.append(step_id)\n",
    "\n",
    "    def get_N_episodes(self) -> int:\n",
    "        \"\"\"Get number of episodes\n",
    "\n",
    "        Returns:\n",
    "            int: number of episodes\n",
    "        \"\"\"\n",
    "        return len(np.unique(self.episode_ids))\n",
    "\n",
    "    def calculate_tail_total_objectives_and_next_baselines(\n",
    "        self,\n",
    "    ) -> Tuple[np.array, float, float]:\n",
    "        \"\"\"Calculate tail total costs and baseline\n",
    "\n",
    "        Returns:\n",
    "            Tuple[np.array, float, float]: tuple of 3 elements tail_total_objectives, baseline, gradent_normalization_constant\n",
    "        \"\"\"\n",
    "\n",
    "        unique_episode_ids = np.unique(self.episode_ids)\n",
    "        running_objectives_series = pd.Series(\n",
    "            index=self.episode_ids, data=self.running_objectives\n",
    "        )\n",
    "\n",
    "        tail_total_objectives = pd.concat(\n",
    "            [\n",
    "                running_objectives_series.loc[i][::-1].cumsum()[::-1]\n",
    "                for i in unique_episode_ids\n",
    "            ]\n",
    "        ).values.reshape(-1)\n",
    "\n",
    "        next_baselines = (\n",
    "            pd.Series(index=self.step_ids, data=tail_total_objectives)\n",
    "            .groupby(level=0)\n",
    "            .mean()\n",
    "            .loc[self.step_ids]\n",
    "            .values.reshape(-1)\n",
    "        )\n",
    "\n",
    "        return tail_total_objectives, next_baselines\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Get length of buffer. The method should be overrided due to inheritance from `torch.utils.data.Dataset`\n",
    "\n",
    "        Returns:\n",
    "            int: length of buffer\n",
    "        \"\"\"\n",
    "        return len(self.observations)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.tensor]:\n",
    "        \"\"\"Get item with id `idx`. The method should be overrided due to inheritance from `torch.utils.data.Dataset`\n",
    "\n",
    "        Args:\n",
    "            idx (int): id of dataset item to return\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, torch.tensor]: dataset item, containing catted observation-action, tail total objective and baselines\n",
    "        \"\"\"\n",
    "\n",
    "        if self.total_objectives is None:\n",
    "            self.baselines = (\n",
    "                self.next_baselines\n",
    "                if self.next_baselines is not None\n",
    "                else np.zeros(shape=len(self.observations))\n",
    "            )\n",
    "\n",
    "            (\n",
    "                self.total_objectives,\n",
    "                self.next_baselines,\n",
    "            ) = self.calculate_tail_total_objectives_and_next_baselines()\n",
    "\n",
    "        observation = torch.tensor(self.observations[idx])\n",
    "        action = torch.tensor(self.actions[idx])\n",
    "\n",
    "        return {\n",
    "            \"observations_actions\": torch.cat([observation, action]),\n",
    "            \"tail_total_objectives\": torch.tensor(self.total_objectives[idx]),\n",
    "            \"baselines\": torch.tensor(self.baselines[idx]),\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def data(self) -> pd.DataFrame:\n",
    "        \"\"\"Return current buffer content in pandas.DataFrame\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: current buffer content\n",
    "        \"\"\"\n",
    "\n",
    "        return pd.DataFrame(\n",
    "            {\n",
    "                \"episode_id\": self.episode_ids,\n",
    "                \"step_id\": self.step_ids,\n",
    "                \"observation\": self.observations,\n",
    "                \"action\": self.actions,\n",
    "                \"running_objective\": self.running_objectives,\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianPDFModel(nn.Module):\n",
    "    \"\"\"Model for REINFORCE algorithm that acts like f(x) + normally distributed noise\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_observation: int,\n",
    "        dim_action: int,\n",
    "        dim_hidden: int,\n",
    "        std: float,\n",
    "        scale_factor: float,\n",
    "        leakyrelu_coef=0.2,\n",
    "    ):\n",
    "        \"\"\"Initialize model.\n",
    "\n",
    "        Args:\n",
    "            dim_observation (int): dimensionality of observation\n",
    "            dim_action (int): dimensionality of action\n",
    "            dim_hidden (int): dimensionality of hidden layer of perceptron (dim_hidden = 4 works for our case)\n",
    "            std (float): standard deviation of noise (\\\\sigma)\n",
    "            action_bounds (np.array): action bounds with shape (dim_action, 2). `action_bounds[:, 0]` - minimal actions, `action_bounds[:, 1]` - maximal actions\n",
    "            scale_factor (float): scale factor for last activation (L coefficient) (see details above)\n",
    "            leakyrelu_coef (float): coefficient for leakyrelu\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim_observation = dim_observation\n",
    "        self.dim_action = dim_action\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.leakyrelu_coef = leakyrelu_coef\n",
    "        self.std = std\n",
    "\n",
    "        self.scale_factor = scale_factor\n",
    "        self.register_parameter(\n",
    "            name=\"scale_tril_matrix\",\n",
    "            param=torch.nn.Parameter(\n",
    "                (self.std * torch.eye(self.dim_action)).float(),\n",
    "                requires_grad=False,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # https://github.com/nikhilbarhate99/PPO-PyTorch/blob/master/PPO.py#L4\n",
    "\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # HINT\n",
    "        #\n",
    "        # Define your perceptron (or its layers) here\n",
    "        #\n",
    "        # TAs used nn.Sequential(...)\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html\n",
    "\n",
    "        # YOUR CODE GOES HERE\n",
    "        self.model = nn.Sequential(\n",
    "                            nn.Linear(self.dim_observation, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, self.dim_action),\n",
    "                            nn.Softmax(dim=-1)\n",
    "                        )\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "    def split_to_observations_actions(\n",
    "        self, observations_actions: torch.FloatTensor\n",
    "    ) -> Tuple[torch.FloatTensor, torch.FloatTensor]:\n",
    "        \"\"\"Split input tensor to tuple of observation(s) and action(s)\n",
    "\n",
    "        Args:\n",
    "            observations_actions (torch.FloatTensor): tensor of catted observations actions to split\n",
    "\n",
    "        Raises:\n",
    "            ValueError: in case if `observations_actions` has dimensinality greater than 2\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.FloatTensor, torch.FloatTensor]: tuple of observation(s) and action(s)\n",
    "        \"\"\"\n",
    "\n",
    "        if len(observations_actions.shape) == 1:\n",
    "            observation, action = (\n",
    "                observations_actions[: self.dim_observation],\n",
    "                observations_actions[self.dim_observation :],\n",
    "            )\n",
    "        elif len(observations_actions.shape) == 2:\n",
    "            observation, action = (\n",
    "                observations_actions[:, : self.dim_observation],\n",
    "                observations_actions[:, self.dim_observation :],\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Input tensor has unexpected dims\")\n",
    "\n",
    "        return observation, action\n",
    "\n",
    "    def log_probs(self, batch_of_observations_actions: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        \"\"\"Get log pdf from the batch of observations actions\n",
    "\n",
    "        Args:\n",
    "            batch_of_observations_actions (torch.FloatTensor): batch of catted observations and actions\n",
    "\n",
    "        Returns:\n",
    "            torch.FloatTensor: log pdf(action | observation) for the batch of observations and actions\n",
    "        \"\"\"\n",
    "\n",
    "        observations, actions = self.split_to_observations_actions(\n",
    "            batch_of_observations_actions\n",
    "        )\n",
    "\n",
    "        scale_tril_matrix = self.get_parameter(\"scale_tril_matrix\")\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # HINT\n",
    "        # You should calculate pdf_Normal(\\\\lambda \\\\mu_theta(observations) + \\\\beta, \\\\lambda ** 2 \\\\sigma ** 2)(actions)\n",
    "        #\n",
    "        # TAs used not NormalDistribution, but MultivariateNormal\n",
    "        # See here https://pytorch.org/docs/stable/distributions.html#multivariatenormal\n",
    "        # YOUR CODE GOES HERE\n",
    "        # print(observations)\n",
    "\n",
    "        # means = self.get_means(observations)\n",
    "        # means = torch.squeeze(means)\n",
    "        # means = self.unscale_from_minus_one_one_to_action_bounds(means)\n",
    "        # _, lambd = self.get_unscale_coefs_from_minus_one_one_to_action_bounds()\n",
    "        # lambd = torch.squeeze(lambd)\n",
    "        # cov_matrix = torch.eye(means.size(0))*(self.std**2)*(lambd**2)\n",
    "        # actions = torch.squeeze(actions)\n",
    "\n",
    "        # print(means.size())\n",
    "        # print(actions.size())\n",
    "        # print(cov_matrix.size())\n",
    "        # beta, lambd = self.get_unscale_coefs_from_minus_one_one_to_action_bounds()\n",
    "        # print(observations)\n",
    "\n",
    "        # log_probs = MultivariateNormal(lambd * self.get_means(observations) + beta, lambd ** 2 * scale_tril_matrix ** 2).log_prob(actions)\n",
    "\n",
    "        action_probs = self.model(observations)\n",
    "        dist = Categorical(action_probs)\n",
    "\n",
    "        action_logprobs = dist.log_prob(actions)\n",
    "        dist_entropy = dist.entropy()\n",
    "\n",
    "\n",
    "        return(action_logprobs)\n",
    "        #-----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    def sample(self, observation: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        \"\"\"Sample action from `MultivariteNormal(lambda * self.get_means(observation) + beta, lambda ** 2 * Diag[self.std] ** 2)`\n",
    "\n",
    "        Args:\n",
    "            observation (torch.FloatTensor): current observation\n",
    "\n",
    "        Returns:\n",
    "            torch.FloatTensor: sampled action\n",
    "        \"\"\"\n",
    "        # action_bounds = self.get_parameter(\"action_bounds\")\n",
    "        # scale_tril_matrix = self.get_parameter(\"scale_tril_matrix\")\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # HINT\n",
    "        # Sample action from `MultivariteNormal(lambda * self.get_means(observation) + beta, lambda ** 2 * Diag[self.std] ** 2)\n",
    "        # YOUR CODE GOES HERE\n",
    "        # means = self.get_means(observation)\n",
    "        # means = self.unscale_from_minus_one_one_to_action_bounds(means)\n",
    "        # beta, lambd = self.get_unscale_coefs_from_minus_one_one_to_action_bounds()\n",
    "\n",
    "        # print(scale_tril_matrix)\n",
    "        # print(observation.size())\n",
    "        # print(torch.eye(observation.size(0))*(lambd**2 * self.std**2))\n",
    "        # m = MultivariateNormal(lambd * self.get_means(observation) + beta, lambd ** 2 * scale_tril_matrix ** 2)\n",
    "        # sampled_action = m.sample()\n",
    "\n",
    "        action_probs = self.model(observation)\n",
    "        dist = Categorical(action_probs)\n",
    "\n",
    "        action = dist.sample()\n",
    "        action_logprob = dist.log_prob(action)\n",
    "\n",
    "\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRSchedulerSwitch:\n",
    "    \"\"\"Callable class that returns True in case ||observation|| <= norm_observation_threshold\"\"\"\n",
    "\n",
    "    def __init__(self, norm_observation_threshold: float) -> None:\n",
    "        \"\"\"Initialize LRSchedulerSwitch.\n",
    "\n",
    "        Args:\n",
    "            norm_observation_threshold (float): threshold for observation norm\n",
    "        \"\"\"\n",
    "        self.norm_observation_threshold = norm_observation_threshold\n",
    "        self.turned_on = False\n",
    "\n",
    "    def __call__(self, observation: np.array) -> bool:\n",
    "        \"\"\"Return True if ||observation|| <= norm_observation_threshold\n",
    "\n",
    "        Args:\n",
    "            observation (np.array): observation\n",
    "\n",
    "        Returns:\n",
    "            bool: ||observation|| <= norm_observation_threshold\n",
    "        \"\"\"\n",
    "\n",
    "        if (\n",
    "            self.turned_on\n",
    "            or np.linalg.norm(observation) <= self.norm_observation_threshold\n",
    "        ):\n",
    "            self.turned_on = True\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "class Optimizer:\n",
    "    \"\"\"Does gradient step for optimizing model weights\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        opt_method: Type[torch.optim.Optimizer],\n",
    "        opt_options: Dict[str, Any],\n",
    "        lr_scheduler_method: Optional[torch.optim.lr_scheduler.LRScheduler] = None,\n",
    "        lr_scheduler_options: Optional[Dict[str, Any]] = None,\n",
    "        lr_scheduler_switch: Callable[[np.array], bool] = lambda _: True,\n",
    "        shuffle: bool = True,\n",
    "    ):\n",
    "        \"\"\"Initialize Optimizer\n",
    "\n",
    "        Args:\n",
    "            model (nn.Module): model which weights we need to optimize\n",
    "            opt_method (Type[torch.optim.Optimizer]): method type for optimization. For instance, `opt_method=torch.optim.SGD`\n",
    "            opt_options (Dict[str, Any]): kwargs dict for opt method\n",
    "            lr_scheduler_method (Optional[torch.optim.lr_scheduler.LRScheduler], optional): method type for LRScheduler. Defaults to None\n",
    "            lr_scheduler_options (Optional[Dict[str, Any]], optional): kwargs for LRScheduler. Defaults to None\n",
    "            lr_scheduler_switch (Callable[[np.array], bool]): callable object for turning on the sheduller. Defaults to lambda _: True\n",
    "            shuffle (bool, optional): whether to shuffle items in dataset. Defaults to True\n",
    "        \"\"\"\n",
    "\n",
    "        self.opt_method = opt_method\n",
    "        self.opt_options = opt_options\n",
    "        self.shuffle = shuffle\n",
    "        self.model = model\n",
    "        self.optimizer = self.opt_method(self.model.parameters(), **self.opt_options)\n",
    "        self.lr_scheduler_method = lr_scheduler_method\n",
    "        self.lr_scheduler_options = lr_scheduler_options\n",
    "        self.lr_scheduler_switch = lr_scheduler_switch\n",
    "        if self.lr_scheduler_method is not None:\n",
    "            self.lr_scheduler = self.lr_scheduler_method(\n",
    "                self.optimizer, **self.lr_scheduler_options\n",
    "            )\n",
    "        else:\n",
    "            self.lr_scheduler = None\n",
    "\n",
    "    def optimize(\n",
    "        self,\n",
    "        objective: Callable[[torch.tensor], torch.tensor],\n",
    "        dataset: IterationBuffer,\n",
    "    ) -> None:\n",
    "        \"\"\"Do gradient step.\n",
    "\n",
    "        Args:\n",
    "            objective (Callable[[torch.tensor], torch.tensor]): objective to optimize\n",
    "            dataset (Dataset): data for optmization\n",
    "        \"\"\"\n",
    "\n",
    "        dataloader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            shuffle=self.shuffle,\n",
    "            batch_size=len(dataset),\n",
    "        )\n",
    "        batch_sample = next(iter(dataloader))\n",
    "        self.optimizer.zero_grad()\n",
    "        objective_value = objective(batch_sample)\n",
    "        objective_value.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        last_observation = dataset.observations[-1]\n",
    "        if self.lr_scheduler_switch(last_observation) and self.lr_scheduler is not None:\n",
    "            self.lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyREINFORCE:\n",
    "    def __init__(\n",
    "        self, model: nn.Module, optimizer: Optimizer, device: str = \"cpu\", is_with_baseline: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize policy\n",
    "\n",
    "        Args:\n",
    "            model (nn.Module): model to optimize\n",
    "            optimizer (Optimizer): optimizer for `model` weights optimization\n",
    "            device (str, optional): device for gradient descent optimization procedure. Defaults to \"cpu\".\n",
    "            is_with_baseline (bool, optional): whether to use baseline in objective function.\n",
    "        \"\"\"\n",
    "\n",
    "        self.buffer = IterationBuffer()\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.is_with_baseline = is_with_baseline\n",
    "\n",
    "    def objective(self, batch: Dict[\"str\", torch.tensor]) -> torch.tensor:\n",
    "        \"\"\"This method computes a proxy objective specifically for automatic differentiation since its gradient is exactly as in REINFORCE\n",
    "\n",
    "        Args:\n",
    "            batch (torch.tensor): batch with catted observations-actions, total objectives and baselines\n",
    "\n",
    "        Returns:\n",
    "            torch.tensor: objective value\n",
    "        \"\"\"\n",
    "\n",
    "        observations_actions = batch[\"observations_actions\"].to(self.device)\n",
    "        tail_total_objectives = batch[\"tail_total_objectives\"].to(self.device)\n",
    "        baselines = batch[\"baselines\"].to(self.device)\n",
    "        N_episodes = self.N_episodes\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # HINT\n",
    "        # Return the surrogate objective value as described above\n",
    "        # YOUR CODE GOES HERE\n",
    "        # for i in range(N_episodes):\n",
    "        # print(observations_actions)\n",
    "        log_probs = self.model.log_probs(observations_actions) !!!!!!!!!!!!!!!!!!!!!! check the shape of log_probs\n",
    "        objective_tensor = (tail_total_objectives - baselines)*log_probs\n",
    "        objective = objective_tensor.sum()/N_episodes\n",
    "\n",
    "        # for m in M:\n",
    "\n",
    "        return objective\n",
    "        #-----------------------------------------------------------------------\n",
    "\n",
    "    def REINFORCE_step(self) -> None:\n",
    "        \"\"\"Do gradient REINFORCE step\"\"\"\n",
    "\n",
    "        self.N_episodes = self.buffer.get_N_episodes()\n",
    "        self.model.to(self.device)\n",
    "        self.optimizer.optimize(self.objective, self.buffer)\n",
    "        self.model.to(\"cpu\")\n",
    "        self.buffer.nullify_buffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarloSimulationScenario:\n",
    "    \"\"\"Run whole REINFORCE procedure\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        simulator: Simulator,\n",
    "        system: InvertedPendulumSystem,\n",
    "        policy: PolicyREINFORCE,\n",
    "        N_episodes: int,\n",
    "        N_iterations: int,\n",
    "        discount_factor: float = 1.0,\n",
    "        termination_criterion: Callable[\n",
    "            [np.array, np.array, float, float], bool\n",
    "        ] = lambda *args: False,\n",
    "    ):\n",
    "        \"\"\"Initialize scenario for main loop\n",
    "\n",
    "\n",
    "        Args:\n",
    "            simulator (Simulator): simulator for computing system dynamics\n",
    "            system (InvertedPendulumSystem): system itself\n",
    "            policy (PolicyREINFORCE): REINFORCE gradient stepper\n",
    "            N_episodes (int): number of episodes in one iteration\n",
    "            N_iterations (int): number of iterations\n",
    "            discount_factor (float, optional): discount factor for running objectives. Defaults to 1\n",
    "            termination_criterion (Callable[[np.array, np.array, float, float], bool], optional): criterion for episode termination. Takes observation, action, running_objective, total_objectove. Defaults to lambda*args:False\n",
    "        \"\"\"\n",
    "\n",
    "        self.simulator = simulator # the simulato and system are the same\n",
    "        self.system = system\n",
    "        self.policy = policy\n",
    "        self.N_episodes = N_episodes\n",
    "        self.N_iterations = N_iterations\n",
    "        self.termination_criterion = termination_criterion\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        self.total_objective = 0\n",
    "        self.total_objectives_episodic = []\n",
    "        self.learning_curve = []\n",
    "        self.last_observations = None\n",
    "\n",
    "    def compute_running_objective(\n",
    "        self, observation: np.array, action: np.array\n",
    "    ) -> float:\n",
    "        \"\"\"Computes running objective\n",
    "\n",
    "        Args:\n",
    "            observation (np.array): current observation\n",
    "            action (np.array): current action\n",
    "\n",
    "        Returns:\n",
    "            float: running objective value\n",
    "        \"\"\"\n",
    "\n",
    "        return self.simulator.current_objective\n",
    "\n",
    "    def run(self) -> None:\n",
    "        \"\"\"Run main loop\"\"\"\n",
    "\n",
    "        eps = 0.1\n",
    "        means_total_objectives = [eps]\n",
    "        for iteration_idx in range(self.N_iterations):\n",
    "            if iteration_idx % 10 == 0:\n",
    "                clear_output(wait=True)\n",
    "            for episode_idx in tqdm(range(self.N_episodes)):\n",
    "                terminated = False\n",
    "\n",
    "                new_action = (\n",
    "                        self.policy.model.sample(torch.tensor(observation).float())\n",
    "                        .detach()\n",
    "                        .cpu()\n",
    "                        .numpy()\n",
    "                    )\n",
    "                self.system.receive_action(new_action)\n",
    "                \n",
    "                while self.simulator.step():\n",
    "                    (\n",
    "                        observation,\n",
    "                        action,\n",
    "                        step_idx,\n",
    "                    ) = self.simulator.get_sim_step_data()\n",
    "\n",
    "                    new_action = (\n",
    "                        self.policy.model.sample(torch.tensor(observation).float())\n",
    "                        .detach()\n",
    "                        .cpu()\n",
    "                        .numpy()\n",
    "                    )\n",
    "                    discounted_running_objective = self.discount_factor ** (\n",
    "                        step_idx\n",
    "                    ) * self.compute_running_objective(observation, new_action)\n",
    "                    self.total_objective += discounted_running_objective\n",
    "\n",
    "                    if not terminated and self.termination_criterion(\n",
    "                        observation,\n",
    "                        new_action,\n",
    "                        discounted_running_objective,\n",
    "                        self.total_objective,\n",
    "                    ):\n",
    "                        terminated = True\n",
    "\n",
    "                    if not terminated:\n",
    "                        self.policy.buffer.add_step_data(\n",
    "                            np.copy(observation),\n",
    "                            np.copy(new_action),\n",
    "                            np.copy(discounted_running_objective),\n",
    "                            step_idx,\n",
    "                            episode_idx,\n",
    "                        )\n",
    "                    self.simulator.receive_action(new_action)\n",
    "                self.simulator.reset()\n",
    "                self.total_objectives_episodic.append(self.total_objective)\n",
    "                self.total_objective = 0\n",
    "            self.learning_curve.append(np.mean(self.total_objectives_episodic))\n",
    "            self.last_observations = pd.DataFrame(\n",
    "                index=self.policy.buffer.episode_ids,\n",
    "                data=self.policy.buffer.observations.copy(),\n",
    "            )\n",
    "            self.last_actions = pd.DataFrame(\n",
    "                index=self.policy.buffer.episode_ids,\n",
    "                data=self.policy.buffer.actions.copy(),\n",
    "            )\n",
    "            self.policy.REINFORCE_step()\n",
    "\n",
    "            means_total_objectives.append(np.mean(self.total_objectives_episodic))\n",
    "            change = (means_total_objectives[-1] / means_total_objectives[-2] - 1) * 100\n",
    "            sign = \"-\" if np.sign(change) == -1 else \"+\"\n",
    "            print(\n",
    "                f\"Iteration: {iteration_idx + 1} / {self.N_iterations}, \"\n",
    "                + f\"mean total cost {round(means_total_objectives[-1], 2)}, \"\n",
    "                + f\"% change: {sign}{abs(round(change,2))}, \"\n",
    "                + f\"last observation: {self.last_observations.iloc[-1].values.reshape(-1)}\",\n",
    "                end=\"\\n\",\n",
    "            )\n",
    "\n",
    "            self.total_objectives_episodic = []\n",
    "\n",
    "    def plot_data(self):\n",
    "        \"\"\"Plot learning results\"\"\"\n",
    "\n",
    "        data = pd.Series(\n",
    "            index=range(1, len(self.learning_curve) + 1), data=self.learning_curve\n",
    "        )\n",
    "        na_mask = data.isna()\n",
    "        not_na_mask = ~na_mask\n",
    "        interpolated_values = data.interpolate()\n",
    "        interpolated_values[not_na_mask] = None\n",
    "        data.plot(marker=\"o\", markersize=3)\n",
    "        interpolated_values.plot(linestyle=\"--\")\n",
    "\n",
    "        plt.title(\"Total cost by iteration\")\n",
    "        plt.xlabel(\"Iteration number\")\n",
    "        plt.ylabel(\"Total cost\")\n",
    "        plt.yscale(\"log\")\n",
    "        plt.show()\n",
    "\n",
    "        theta_ax, dot_theta_ax = pd.DataFrame(\n",
    "            data=self.last_observations.loc[0].values\n",
    "        ).plot(\n",
    "            xlabel=\"Step Number\",\n",
    "            title=\"Observations in last iteration\",\n",
    "            legend=False,\n",
    "            subplots=True,\n",
    "            grid=True,\n",
    "        )\n",
    "        theta_ax.set_ylabel(\"angle\")\n",
    "        dot_theta_ax.set_ylabel(\"angular velocity\")\n",
    "\n",
    "        actions_ax = pd.DataFrame(\n",
    "            data=self.last_actions.loc[0].values\n",
    "        ).plot(\n",
    "            xlabel=\"Step Number\",\n",
    "            title=\"Actions in last iteration\",\n",
    "            legend=False,\n",
    "            grid=True,\n",
    "        )\n",
    "        actions_ax.set_ylabel(\"action\")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 14\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "system = InvertedPendulumSystem()\n",
    "## DO NOT CHANGE THE PARAMS OF SIMULATOR.\n",
    "simulator = Simulator(\n",
    "    system, N_steps=1800, step_size=0.003, state_init=np.array([np.pi, 0.0])\n",
    ")\n",
    "model = GaussianPDFModel(\n",
    "    dim_observation=system.dim_observation,\n",
    "    dim_action=system.dim_action,\n",
    "    action_bounds=np.array([[-20, 20]]),\n",
    "    #---------------------------------------------------------------------------\n",
    "    # YOUR CODE GOES HERE\n",
    "    scale_factor=1,  # TRY TO FIND scale_factor EMPIRICALLY\n",
    "    dim_hidden=3, # TRY TO FIND dim_hidden EMPIRICALLY\n",
    "    std=0.01 # TRY TO FIND STD EMPIRICALLY\n",
    "    #---------------------------------------------------------------------------\n",
    ")\n",
    "\n",
    "# optimizer = Optimizer(\n",
    "#     model=model,\n",
    "#     opt_method=torch.optim.Adam,\n",
    "#     #---------------------------------------------------------------------------\n",
    "#     # YOUR CODE GOES HERE\n",
    "#     opt_options=dict(lr=0.05) # TRY TO FIND lr EMPIRICALLY\n",
    "#     #---------------------------------------------------------------------------\n",
    "# )\n",
    "## Or if you want to use scheduler then initialize optimizer, via, for instance\n",
    "lr_scheduler_fading_coeff = 1\n",
    "optimizer = Optimizer(\n",
    "    model=model,\n",
    "    opt_method=torch.optim.Adam,\n",
    "    opt_options=dict(lr=0.025, betas=(0.8, 0.9)),\n",
    "    shuffle=False,\n",
    "    lr_scheduler_method=torch.optim.lr_scheduler.MultiplicativeLR,\n",
    "    lr_scheduler_options={\n",
    "        \"lr_lambda\": lambda iteration: 1\n",
    "        / np.sqrt((iteration / lr_scheduler_fading_coeff) ** 2 + 1)\n",
    "    },\n",
    "    lr_scheduler_switch=LRSchedulerSwitch(norm_observation_threshold=0.1),\n",
    ")\n",
    "#\n",
    "# BELEIVE US! YOU CAN SOLVE THIS TASK WITHOUT SCHEDULER\n",
    "\n",
    "policy = PolicyREINFORCE(model, optimizer, is_with_baseline=True)\n",
    "\n",
    "\n",
    "# This termination criterion never terminates episodes\n",
    "trivial_terminantion_criterion = lambda *args: False\n",
    "\n",
    "## EXAMPLE. This termination criterion terminates episode if observation norm >= 20\n",
    "#\n",
    "# termination_criterion = (\n",
    "#     lambda observation, action, running_objective, total_objective: (\n",
    "#         np.linalg.norm(observation) >= 20\n",
    "#     )\n",
    "# )\n",
    "#\n",
    "# DO NOT USE TERMINATION CRITERION OTHER THAN trivial_termination_criterion\n",
    "\n",
    "\n",
    "scenario = MonteCarloSimulationScenario(\n",
    "    simulator=simulator,\n",
    "    system=system,\n",
    "    policy=policy,\n",
    "    #---------------------------------------------------------------------------\n",
    "    # YOUR CODE GOES HERE\n",
    "    N_episodes=6, # Increasing the number of episodes stabilizes learning, but you can manage it with N_episodes=1\n",
    "    N_iterations=400, # You can change the number of iterations if you want\n",
    "    #---------------------------------------------------------------------------\n",
    "    termination_criterion=trivial_terminantion_criterion,\n",
    "    discount_factor=1, # do not change this\n",
    ")\n",
    "\n",
    "try:\n",
    "    scenario.run()\n",
    "except KeyboardInterrupt:\n",
    "    clear_output(wait=True)\n",
    "    scenario.plot_data()\n",
    "\n",
    "clear_output(wait=True)\n",
    "scenario.plot_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
